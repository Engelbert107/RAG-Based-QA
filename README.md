# RAG-Based Question Answering from PDF

This project allows users to ask questions directly about PDF files and receive clear, relevant answers. It implements a Retrieval-Augmented Generation (RAG) pipeline: text is extracted and preprocessed from PDFs, embedded into a ChromaDB vector store, and at query time, relevant chunks are retrieved and combined with a lightweight LLM (TinyLlama) to generate accurate, context-aware responses.

## ðŸ”„ How It Works

```mermaid
flowchart TD
    A[ðŸ“„ PDF Document] --> B[ðŸ” Text Extraction & Cleaning]
    B --> C[ðŸ“¦ Embedding with HuggingFace]
    C --> D[ðŸ—‚ï¸ Store in ChromaDB]
    E[â“ User Query] --> F[ðŸ”Ž Retrieve Relevant Chunks from ChromaDB]
    F --> G[ðŸ¤– TinyLlama LLM]
    D --> F
    G --> H[âœ… Context-Aware Answer]

